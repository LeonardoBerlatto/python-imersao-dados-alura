# -*- coding: utf-8 -*-
"""aula_05.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1t0I9qnXRsGDWx0pPpYTYXQUywnDq3ftO

## Bibliotecas
"""

import pandas
import matplotlib.pyplot as plot
import seaborn as sns
import numpy
import sklearn

"""## Lendo os Dados"""

dataset = pandas.read_csv('https://raw.githubusercontent.com/alura-cursos/imersao-dados-2-2020/master/MICRODADOS_ENEM_2019_SAMPLE_43278.csv')

"""### Variáveis importantes"""

tests = ['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_MT', 'NU_NOTA_LC','NU_NOTA_REDACAO']

dataset['NU_NOTA_TOTAL'] = dataset[tests].sum(axis=1)

students_without_zero_score = dataset.query('NU_NOTA_TOTAL != 0')

"""## Modelo de Previsão"""

# Varíaveis funcionais ou independentes (x)
input_tests = ['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC','NU_NOTA_REDACAO']

# Varíavel dependente (y)
output_test = 'NU_NOTA_MT'

# Removendo notas NaN
students_without_zero_score = students_without_zero_score[tests].dropna()

input_scores = students_without_zero_score[input_tests]
output_score = students_without_zero_score[output_test]

input_scores, output_score

# Nomenclatura padrão
x = input_scores
y = output_score

from sklearn.model_selection import train_test_split

# Número aleatório para selecionar sempre os mesmos dados
SEED = 1313

x_train, x_test, y_train, y_test = train_test_split(x, 
                                                    y, 
                                                    test_size=0.25)


print(f"Notas no total: {len(x)} | Notas de treino: {len(x_train)} | Notas de teste: {len(x_test)}")

from sklearn.svm import LinearSVR

model = LinearSVR(random_state = SEED)

model.fit(x_train, y_train)
math_predictions = model.predict(x_test)

from sklearn.metrics import mean_squared_error

mean_squared_error(y_test, math_predictions)

"""## Insights da Aula

### Predição com Decision Tree
"""

from sklearn.tree import DecisionTreeRegressor


x_train, x_test, y_train, y_test = train_test_split(x, 
                                                    y, 
                                                    test_size=0.25)


tree_model = DecisionTreeRegressor(max_depth = 3)
tree_model.fit(x_train, y_train)
tree_model_math_predictions = tree_model.predict(x_test)
mean_squared_error(y_test, tree_model_math_predictions)

from sklearn.model_selection import cross_validate
from sklearn.model_selection import KFold

number_of_splits = KFold(n_splits=10, shuffle=True)

tree_model = DecisionTreeRegressor(max_depth = 5)

result = cross_validate(tree_model, x, y, cv=number_of_splits, scoring='neg_mean_squared_error') # retorno negativo, pois quanto maior, melhor

absolute_results = result['test_score'] * -1

score_mean = absolute_results.mean()
print(score_mean)

"""### Desvio padrão e Intervalo de Confiança do Model"""

from scipy import stats

def print_confidence_interval(results):
  absolute_results = results['test_score'] * -1
  score_mean = absolute_results.mean()
  results_standard_deviation = absolute_results.std()

  lower_limit, upper_limit = stats.norm.interval(0.95, loc=score_mean, scale=results_standard_deviation)

  print(f'Intervalo de confiança obtido no modelo: {lower_limit} - {upper_limit}')

print_confidence_interval(result)